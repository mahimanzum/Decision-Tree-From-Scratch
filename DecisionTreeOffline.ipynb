{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecisionTreeOffline.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"6FN03lfLlZP5","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from numpy import log2 as log\n","from sklearn.preprocessing import Imputer\n","eps = np.finfo(float).eps\n","\n","from sklearn.utils import shuffle\n","\n","\n","#print(eps)\n","dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n","       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n","       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n","       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}\n","\n","df = pd.DataFrame(dataset,columns= list(dataset.keys()))\n","\n","\n","def preProcessTelco():\n","    #https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Data_Preprocessing-Missing-Data-Categorical-Data.php\n","    #imputer = Imputer(missing_values ='NaN', strategy = 'mean', axis = 0)\n","    df = pd.read_csv('telco.csv')\n","    df.dropna(subset = ['Churn'])\n","    df.drop('customerID', axis = 1, inplace=True)\n","    df.drop('TotalCharges', axis = 1, inplace=True)\n","    df['SeniorCitizen'] = df['SeniorCitizen'].astype(object)\n","    df.loc[:, 'SeniorCitizen'].replace([1,0], [\"Yes\",\"No\"], inplace=True)\n","    print(df.dtypes)\n","    #imputer = imputer.fit(df)\n","    #imputer_data = Imputer.fit_transform(df['tenure'].values)\n","    for i in range(len(df.keys())):\n","        if(df.dtypes[i] == 'int64' or df.dtypes[i] == 'float64'):\n","            mean = df[df.keys()[i]].mean()\n","            df[df.keys()[i]] = df[df.keys()[i]].fillna(mean)\n","            print(df.keys()[i])\n","    #if still there is missing data than is is catagorical so drop that row  \n","    st = []\n","    for i in range(len(df.keys())):\n","        st.append(df.keys()[i])\n","    df.dropna(subset = st)\n","    df = shuffle(df).reset_index(drop=True)\n","    pos_df = df[df['Churn'] == 'Yes']\n","    neg_df = df[df['Churn'] == 'No']\n","    min_length = min(len(pos_df), len(neg_df))\n","    print(len(pos_df),len(neg_df))\n","    \n","    #for same pos neg total\n","    #test_df = pd.concat([pos_df[-(int)(0.2*min_length):],neg_df[-(int)(0.2*min_length):]])\n","    #test_df = test_df.reset_index(drop=True)\n","    #df = pd.concat([pos_df[:(int)(0.8*min_length)],neg_df[0:(int)(0.8*min_length)]])\n","    #df = df.reset_index(drop=True)\n","    \n","    #for same pos neg in train and test\n","    test_df = pd.concat([pos_df[-(int)(0.2*len(pos_df)):],neg_df[-(int)(0.2*len(neg_df)):]])\n","    test_df = test_df.reset_index(drop=True)\n","    df = pd.concat([pos_df[:(int)(0.8*len(pos_df))],neg_df[0:(int)(0.8*len(neg_df))]])\n","    df = df.reset_index(drop=True)\n","    \n","    #test_df = df[-int((0.2)*len(df)):]\n","    #df = df[:int(0.8*len(df))]\n","    \n","    #df = shuffle(df)\n","    return shuffle(df), shuffle(test_df)\n","\n","def preProcessCreditcard():\n","    #https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Data_Preprocessing-Missing-Data-Categorical-Data.php\n","    #imputer = Imputer(missing_values ='NaN', strategy = 'mean', axis = 0)\n","    df = pd.read_csv('creditcard.csv')\n","    #df = pd.DataFrame(dataset,columns= list(dataset.keys()))\n","    \n","    df['Class'] = df['Class'].astype(object)\n","    df.dropna(subset = ['Class'])\n","    #print(df.dtypes)\n","    df.drop('Time', axis = 1, inplace=True)\n","    #df['Class'] = df['Class'].map({'1':\"Yes\", '0':\"No\"})\n","    df.loc[:, 'Class'].replace([1,0], [\"Yes\",\"No\"], inplace=True)\n","    print(df.dtypes)\n","    #print(df.head)\n","    for i in range(len(df.keys())):\n","        if(df.dtypes[i] == 'int64' or df.dtypes[i] == 'float64'):\n","            mean = df[df.keys()[i]].mean()\n","            df[df.keys()[i]] = df[df.keys()[i]].fillna(mean)\n","            print(df.keys()[i])\n","            \n","    #if still there is missing data than is is catagorical so drop that row  \n","    st = []\n","    for i in range(len(df.keys())):\n","        st.append(df.keys()[i])\n","    df.dropna(subset = st)\n","    df = shuffle(df).reset_index(drop=True)\n","    pos_df = df[df['Class'] == 'Yes'].reset_index(drop=True)\n","    neg_df = df[df['Class'] == 'No'].reset_index(drop=True)\n","    min_length = min(len(pos_df), len(neg_df))\n","    print(len(pos_df),len(neg_df))\n","    #test_df = pd.concat([pos_df[-(int)(0.2*min_length):],neg_df[-(int)(0.2*min_length):]])\n","    #test_df = test_df.reset_index(drop=True)\n","    \n","    df = pd.concat([pos_df,neg_df[:20000]]).reset_index(drop=True)\n","    df = shuffle(df).reset_index(drop=True)\n","    \n","    test_df = df[-(int)(0.2*len(df)):].reset_index(drop=True)\n","    df = df[:(int)(0.8*len(df))].reset_index(drop=True)\n","    print(len(df))\n","    print(len(test_df))\n","    \n","    test_df = pd.concat([pos_df[-(int)(0.2*min_length):],neg_df[-(int)(0.2*min_length):]]).reset_index(drop=True)\n","    df = pd.concat([pos_df[:(int)(0.8*min_length)],neg_df[:(int)(0.8*min_length)]]).reset_index(drop=True)\n","    \n","    return shuffle(df), shuffle(test_df)\n","\n","def preProcessAdult():\n","    cols = []\n","    for i in range(14):\n","        cols.append(\"feature\"+str(i))\n","    cols.append(\"Class\")\n","    df = pd.read_csv('adult_train.csv', names = cols, header = None)\n","    test_df = pd.read_csv('adult_test.csv', names = cols, header = None)\n","    test_df = test_df[1:]\n","    test_len = len(test_df)\n","    df = pd.concat([df,test_df]).reset_index(drop=True)\n","    #remove all rows with question marks\n","    df = df[df.apply(lambda x: sum([x_=='?' for x_ in x])==0, axis=1)]\n","    df = df[df.apply(lambda x: sum([x_==' ?' for x_ in x])==0, axis=1)]\n","    df = df[df.apply(lambda x: sum([x_== np.nan for x_ in x])==0, axis=1)]\n","    df = df.reset_index(drop=True)\n","    #df = df[(str(df) != \"?\").all(axis=1)]\n","    #df = df[(df != ' ?').all(axis=1)]\n","    #df = df[(df != np.nan).all(axis=1)]\n","    #test_df = test_df[(test_df != '?').all(axis=1)]\n","    \n","    df.dropna(subset = ['Class'])\n","    df['Class'] = df['Class'].astype(object)\n","    #df.feature0 = df.feature0.astype(float).fillna(0.0)\n","    #df['feature0'] = df['feature0'].astype(int)\n","    \n","    #to convert feature 0\n","    frm = []\n","    to = []\n","    for i in range(100):\n","        frm.append(str(i))\n","        to.append(i)\n","    frm.append('|1x3 Cross validator')\n","    to.append(0)\n","    df = df[df.apply(lambda x: sum([x_=='?' for x_ in x])==0, axis=1)]\n","    df = df[df.apply(lambda x: sum([x_==' ?' for x_ in x])==0, axis=1)]\n","    df = df[df.apply(lambda x: sum([x_== np.nan for x_ in x])==0, axis=1)]\n","    \n","    #df.loc[:, 'feature0'].replace(frm,to, inplace=True)\n","    df.dropna(subset = cols)\n","    \n","    #mean = df['feature0'].mean()\n","    #mean = float(int(mean))\n","    df['feature0'] = df['feature0'].astype(float)#.fillna(mean)\n","    mean = df['feature0'].mean()\n","    df['feature0'] = df['feature0'].astype(float)#.fillna(mean)\n","    print(to)\n","    print(\"start\")\n","    print(df['feature0'].unique())\n","    print(\"end\")\n","    #print(df.head)\n","    #print(df['Class'].unique())\n","    df.loc[:, 'Class'].replace([\" <=50K\",\" >50K\"], [\"Yes\",\"No\"], inplace=True)\n","    df.loc[:, 'Class'].replace([\" <=50K.\",\" >50K.\"], [\"Yes\",\"No\"], inplace=True)\n","    #print(df.dtypes)\n","    #print(df.head)\n","    for i in range(len(df.keys())):\n","        if(df.dtypes[i] == 'int64' or df.dtypes[i] == 'float64'):\n","            mean = df[df.keys()[i]].mean()\n","            df[df.keys()[i]] = df[df.keys()[i]].fillna(mean)\n","            #print(df.keys()[i])\n","            \n","    #if still there is missing data than is is catagorical so drop that row  \n","    st = []\n","    for i in range(len(df.keys())):\n","        st.append(df.keys()[i])\n","    df.dropna(subset = st)\n","    test_len = (int)(0.2*len(df))\n","    df = shuffle(df).reset_index(drop=True)\n","    test_df = df[-test_len:].reset_index(drop=True)\n","    df = df[:-test_len].reset_index(drop=True)\n","    print(df.dtypes)\n","    \n","    return shuffle(df), shuffle(test_df)\n","def preProcessOnline():\n","    df = pd.read_csv(\"online1_data.csv\")\n","    print(df.dtypes)\n","\n","#df, test_df = preProcessTelco()\n","#df, test_df = preProcessCreditcard()\n","#df, test_df = preProcessAdult()\n","preProcessOnline\n","#print(df.dtypes)\n","used = {}\n","uniqueFeaturesLables = {}\n","for attr in list(df.keys()):\n","    uniqueFeaturesLables[attr] = []\n","#print(df['Taste'])\n","\n","\n","for attr in list(df.keys()):\n","    for val in df[attr]:\n","        if(val not in uniqueFeaturesLables[attr]):\n","            uniqueFeaturesLables[attr].append(val)\n","'''\n","for attr in list(df.keys()):\n","    print(uniqueFeaturesLables[attr])\n","print(len(uniqueFeaturesLables))\n","'''\n","#df = pd.DataFrame(dataset)\n","#print(df.dtypes)\n","#print(df)\n","#print(test_df.dtypes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9dJ7Op3qlZQb","colab_type":"code","colab":{}},"cell_type":"code","source":["print(len(test_df))\n","print(len(df))\n","#test_df[feature3].value_counts()[nan]\n","'''\n","for i in test_df.keys():\n","    print(i)\n","    print(test_df[i].unique())\n","'''\n","#test_df.head\n","#test 196\n","#train 786\n","#print(df[:400])\n","#print((df.iloc[10]['Class']))\n","#print(len(df.iloc[10]['Class']))\n","\n","#print((test_df.iloc[10]['Class']))\n","#print(len(test_df.iloc[10]['Class']))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W6opLoPolZQt","colab_type":"code","colab":{}},"cell_type":"code","source":["def Entropy(df, attr = None):\n","    label = df.keys()[-1]\n","    values = df[label].unique()\n","    #print(values)\n","    entropy = 0.0\n","    \n","    for value in values:\n","        fraction = df[label].value_counts()[value]/len(df[label])\n","        entropy += -fraction*np.log2(fraction)\n","    if attr == None:\n","        return (entropy)\n","    entropy = 0.0\n","    final_entropy = 0\n","    attr_variables = df[attr].unique()\n","    for variable in attr_variables:\n","        entropy = 0.0\n","        for value in values:\n","            num = len(df[attr][df[attr] == variable][df[label] ==value])\n","            den = len(df[attr][df[attr] == variable])\n","            fraction = num/(den+eps)\n","            #print(den)\n","            entropy += -fraction*log(fraction+eps)\n","            fraction2 = den/len(df)\n","            final_entropy += fraction2*entropy\n","    #print(attr_variables)\n","    return (final_entropy)\n","\n","def ContEntropy(df, attr):\n","    label = df.keys()[-1]\n","    values = df[label].unique()\n","    entropy = 0.0\n","    total_yes = 0\n","    total_no = 0\n","    now_yes = 0\n","    now_no = 0\n","    total = 0\n","    for value in values:\n","        fraction = df[label].value_counts()[value]/len(df[label])\n","        if value == 'Yes':\n","            total_yes = df[label].value_counts()[value]\n","        else:\n","            total_no = df[label].value_counts()[value]\n","        entropy += -fraction*np.log2(fraction)\n","    total = total_yes+total_no\n","    entropy = 0.0\n","    final_entropy = 100\n","    partition_value = 0\n","    #attr_variables = df[attr].unique()\n","    attr_variables = []\n","    for i in range(len(df)):\n","        #print(df[attr][i])\n","        #print(i)\n","        attr_variables.append((df[attr][i],df[label][i]))\n","    attr_variables = sorted(attr_variables,key = lambda element : element[0])\n","    #print((attr_variables))\n","    \n","    for i in range(len(attr_variables)):\n","        entropy = 0.0\n","        if(attr_variables[i][1]=='Yes'):\n","            now_yes += 1\n","        else:\n","            now_no += 1\n","        other_yes = total_yes - now_yes\n","        other_no = total_no - now_no\n","        if(i+1 != len(attr_variables) and attr_variables[i][0]==attr_variables[i+1][0]):\n","            continue\n","        entropy += -((now_yes+now_no)/(total+eps))*(((now_yes/(now_yes+now_no+eps))*log(now_yes/(now_yes+now_no+eps)))+((now_no/(now_yes+now_no+eps))*log(now_no/(now_yes+now_no+eps))))\n","        entropy += -((other_yes+other_no)/(total+eps))*(((other_yes/(other_yes+other_no+eps))*log(other_yes/(other_yes+other_no+eps)))+((other_no/(other_yes+other_no+eps))*log(other_no/(other_yes+other_no+eps))))\n","        if(entropy <= final_entropy):\n","            partition_value = attr_variables[i][0]\n","        final_entropy = min(final_entropy, entropy)\n","    '''\n","    \n","    for variable in attr_variables:\n","        entropy = 0.0\n","        for value in values:\n","            num = len(df[attr][df[attr] <= variable][df[label] ==value])\n","            den = len(df[attr][df[attr] <= variable])\n","            fraction = num/(den+eps)\n","            entropy += -fraction*log(fraction+eps)\n","        if(entropy <= final_entropy):\n","            partition_value = variable\n","        final_entropy = min(final_entropy, entropy)\n","    '''\n","    return final_entropy, partition_value\n","\n","def find_winner(df,used):\n","    #print(df.keys())\n","    #Entropy_att = []\n","    gain = []\n","    chose = []\n","    partition = []\n","    #used = list(used)\n","    for key in df.keys()[:-1]:\n","        #print(key)\n","        #print(df.dtypes[key])\n","        if key not in used:\n","            chose.append(key)\n","            if df.dtypes[(key)] == 'object':\n","                #print(\"dhukse\")\n","                gain.append(Entropy(df)-Entropy(df,key))\n","                partition.append(\"Null\")\n","            else:\n","                #print(\"dhuke nai\")\n","                #print(\"ashse\" + str(key))\n","                entrpy , parttn = ContEntropy(df,key)\n","                gain.append(Entropy(df)-entrpy)\n","                partition.append(parttn)\n","    gain = np.array(gain)\n","    idx =  np.argmax(gain)\n","    #print(partition)\n","    return chose[idx],partition[idx] \n","  \n","def get_subtable(df,index,value):\n","    return df[df[index] == value].reset_index(drop=True)\n","\n","def get_subtable_cont(df,index,value):\n","    less_table = df[df[index] <= value].reset_index(drop=True)\n","    greater_table = df[df[index] > value].reset_index(drop=True)\n","    return less_table, greater_table\n","\n","find_winner(df, [])\n","\n","#ContEntropy(df, 'tenure')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9HvyvD9DlZRB","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def makeTree(df,depth, max_depth, used, tree = None):\n","    #print(\"used = \")\n","    #print(used)\n","    Class = df.keys()[-1]\n","    #print(Class)\n","    #if(len(df[Class].unique())==1):\n","    #    return str(df[Class].unique()[0])\n","    #print(Class)\n","    \n","    values = df[Class].unique()\n","    x = 0\n","    majority_class = values[0]\n","    for value in values:\n","        #print(value)\n","        y = df[Class].value_counts()[value]\n","        if(y>x):\n","            x = y\n","            majority_class = value\n","    if depth==len(uniqueFeaturesLables)-1 or depth == max_depth:\n","        return str(majority_class)\n","    \n","    partition_index, cont_partition_val = find_winner(df, used)\n","    \n","    \n","    #print(partition_index)\n","    #partition_values= np.unique(df[partition_index])\n","    partition_values = uniqueFeaturesLables[partition_index]\n","    if tree == None:\n","        tree = {}\n","        tree[partition_index] = {}\n","    #values = uniqueFeaturesLables[Class]\n","    \n","    if cont_partition_val == \"Null\":\n","        \n","        for value in partition_values:\n","            table = get_subtable(df, partition_index, value)\n","            ClassVal,cnt = np.unique(table[Class],return_counts=True)\n","            #print(\"ClassVal = \"+ ClassVal);\n","            if len(cnt) == 1:\n","                tree[partition_index][value] = ClassVal[0]\n","            elif(len(table) == 0):\n","                tree[partition_index][value] = str(majority_class)\n","            else:\n","                used.append(partition_index)\n","                tree[partition_index][value] = makeTree(table,depth+1,max_depth,used)\n","                used.pop()\n","    else:\n","        less_table, greater_table = get_subtable_cont(df, partition_index, cont_partition_val)\n","        less_ClassVal,less_cnt = np.unique(less_table[Class],return_counts=True)\n","        greater_ClassVal,greater_cnt = np.unique(less_table[Class],return_counts=True)\n","        done = 0\n","        #print(\"ClassVal = \"+ ClassVal);\n","        if(done == 0):\n","            if len(less_cnt) == 1:\n","                tree[partition_index][\"<=\"+str(cont_partition_val)] = less_ClassVal[0]\n","            elif(len(less_table) == 0):\n","                tree[partition_index][\"<=\" + str(cont_partition_val)] = str(majority_class)\n","            else:\n","                print(used)\n","                print(partition_index)\n","                used.append(partition_index)\n","                tree[partition_index][\"<=\" + str(cont_partition_val)] = makeTree(less_table, depth+1,max_depth,used)\n","                used.pop()\n","        if(done == 0):\n","            if len(greater_cnt) == 1:\n","                tree[partition_index][str(cont_partition_val)] = greater_ClassVal[0]\n","            elif(len(greater_table) == 0):\n","                tree[partition_index][str(cont_partition_val)] = str(majority_class)\n","            else:\n","                used.append(partition_index)\n","                tree[partition_index][str(cont_partition_val)] = makeTree(greater_table, depth+1,max_depth,used)\n","                used.pop()\n","            \n","    return tree\n","    \n","    \n","tree = makeTree(df,0,200,[])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VJVpi_avlZRQ","colab_type":"code","colab":{}},"cell_type":"code","source":["import pprint\n","#tree = makeTree(df, depth = 0, max_depth = 2)\n","pprint.pprint(tree)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X7Y3ei_rlZSC","colab_type":"code","colab":{}},"cell_type":"code","source":["import pprint\n","#tree = makeTree(df, depth = 0, max_depth = 2)\n","pprint.pprint(tree)\n","\n","unique_labels = df[df.keys()[-1]].unique()\n","unique_labels\n","print(unique_labels)\n","def predict(tree, inst):\n","    #pprint.pprint(tree)\n","    for nodes in tree.keys():\n","        if(nodes in unique_labels):\n","            return nodes\n","        value = inst[nodes]\n","        \n","        if df.dtypes[nodes] != 'object':\n","            #print(list(tree[nodes].keys())[0])\n","            \n","            if list(tree[nodes].keys())[0][0]=='<':\n","                less_value = list(tree[nodes].keys())[0]\n","                greater_value = list(tree[nodes].keys())[1]\n","            else:\n","                less_value = list(tree[nodes].keys())[1]\n","                greater_value = list(tree[nodes].keys())[0]\n","                 \n","            \n","            if((df.dtypes[nodes] == 'int64' and inst[nodes] <= int(less_value[2:])) or (df.dtypes[nodes] == 'float64' and inst[nodes] <= float(less_value[2:]))):\n","                tree = (tree[nodes][less_value])\n","            else:\n","                tree = (tree[nodes][greater_value])\n","        else:\n","            tree = tree[nodes][value]\n","        #print(\"nodes = \" + nodes)\n","        #print(\"value = \" + value)\n","        #tree = (tree[nodes][value])\n","        \n","        \n","        prediction = 'No'\n","        if type(tree) is dict:\n","            prediction = predict(tree, inst)\n","        else:\n","            prediction = tree\n","            break;  \n","    return prediction\n","    if prediction=='Yes':\n","        return 1.0\n","    return -1.0\n","\n","inst = df.iloc[6]\n","print(inst)\n","#print(inst['Taste'])\n","print(predict(tree, inst))\n","#for i in range(10):\n","    #print(predict(tree, df.iloc[i]))\n","#for i in range(10):\n","    #print(predict(tree, df.iloc[i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hkKlkYl6lZS0","colab_type":"code","colab":{}},"cell_type":"code","source":["#only to run ada-boost\n","\n","#returns a vector of trees and weights of trees \n","from numpy.random import choice\n","feature_list = df.keys()\n","\n","def make_feedable(data):\n","    data_dic = {}\n","    for i in feature_list:\n","        data_dic[i] = []\n","    for j in feature_list:\n","        for i in data:\n","            data_dic[j].append(df[j][i])\n","    #print(data_dic)\n","    dic_df = pd.DataFrame(data_dic,columns= feature_list)\n","    return dic_df\n","\n","\n","def AdaBoost(df, K = 5):\n","    data_weights = []\n","    Hypothesis_vector = []\n","    Hypothesis_weights = []\n","    list_of_candidates = []\n","    sampled = []\n","    for i in range(len(df)):\n","        data_weights.append(1.0/len(df))\n","        sampled.append(i)\n","    data_weights = np.array(data_weights)\n","        #print(data_weights[i])\n","    #print(data_weights.values())\n","    \n","    for j in range(len(df)):\n","        list_of_candidates.append(j)\n","        \n","    i = 0\n","    #change it\n","    new_df = df.copy()\n","    while (i<K):\n","        if i != 0:\n","            sampled = choice(list_of_candidates, int(len(df)), p=list(data_weights))\n","            new_df = make_feedable(sampled)\n","        tree = makeTree(new_df, 0, 1,[])\n","        \n","        pprint.pprint(tree)\n","        #print(sampled)\n","        print(np.sum(data_weights))\n","        error = 0.0\n","        for j in range(len(df)):\n","            prediction = predict(tree, df.iloc[j])\n","            if(prediction != df.iloc[j][df.keys()[-1]]):\n","                error = error + data_weights[j]\n","        print(error)\n","        if error>0.5:\n","            #print(i)\n","            continue\n","        for j in range(len(new_df)):\n","            prediction = predict(tree, new_df.iloc[sampled[j]])\n","            if(prediction == new_df.iloc[sampled[j]][new_df.keys()[-1]]):\n","                data_weights[sampled[j]] = (data_weights[sampled[j]]*error)/(1-error+eps)\n","        #normalization_step\n","        print(np.sum(data_weights))\n","        data_weights /= np.sum(data_weights)\n","            \n","        Hypothesis_weights.append(log((1-error)/error))\n","        Hypothesis_vector.append(tree)\n","        pprint.pprint(tree)\n","        print(\"at  \" + str(i) + \"  accuracy = \"+ str(1-error))\n","        #https://stackoverflow.com/questions/3679694/a-weighted-version-of-random-choice\n","        \n","       #sampled = choice(list_of_candidates, int(len(df)), p=list(data_weights))\n","        i = i + 1\n","        \n","    return Hypothesis_vector, Hypothesis_weights    \n","#Hypothesis_vector, Hypothesis_weights = AdaBoost(df, 20)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gfvB0KgulZTO","colab_type":"code","colab":{}},"cell_type":"code","source":["#only after running adaboost\n","\n","def final_prediction(Hypothesis_vector, Hypothesis_weights, instance):\n","    outpt = 0.0\n","    for i in range(len(Hypothesis_vector)-15):\n","        if(predict( Hypothesis_vector[i], instance) == 'Yes'):\n","           outpt = outpt + Hypothesis_weights[i]\n","        else:\n","           outpt = outpt - Hypothesis_weights[i]\n","    if outpt>=0:\n","        return \"Yes\"\n","    else:\n","        return \"No\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2QR0SAczlZTe","colab_type":"code","colab":{}},"cell_type":"code","source":["#for i in range(len(Hypothesis_vector)):\n","    #print(final_prediction(Hypothesis_vector, Hypothesis_weights, df.iloc[i]))\n","#    pprint.pprint(Hypothesis_vector[i])\n","\n","def accuracy(df):\n","    correct = 0\n","    for i in range(len(df)):\n","        #pred = final_prediction(Hypothesis_vector, Hypothesis_weights, df.iloc[i])\n","        pred = predict(tree, df.iloc[i])\n","        if df.iloc[i][df.keys()[-1]] == pred:\n","            correct =  correct+1\n","    return correct/len(df)\n","print(len(df))\n","\n","#print(accuracy(df))\n","#print(accuracy(test_df))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0AC6NbJelZTx","colab_type":"code","colab":{}},"cell_type":"code","source":["#set 1 0.7265415549597856\n","#set 2 0.8928571428571429\n","#set 3 0.8030340253040167"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_5rONQEulZT9","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def calc_all(df):\n","    true_pos = 0\n","    false_pos = 0\n","    true_neg = 0\n","    false_neg = 0\n","    for i in range(len(df)):\n","        if(i%100==0):\n","            print(i)\n","        #pred = final_prediction(Hypothesis_vector, Hypothesis_weights, df.iloc[i])\n","        pred = predict(tree, df.iloc[i])\n","        if df.iloc[i][df.keys()[-1]]==\"Yes\" and pred==\"Yes\":\n","            true_pos+=1\n","        if df.iloc[i][df.keys()[-1]]==\"Yes\" and pred==\"No\":\n","            false_neg+=1\n","        if df.iloc[i][df.keys()[-1]]==\"No\" and pred==\"Yes\":\n","            false_pos+=1\n","        if df.iloc[i][df.keys()[-1]]==\"No\" and pred==\"No\":\n","            true_neg+=1\n","    return true_pos, false_pos, true_neg, false_neg\n","true_pos, false_pos, true_neg, false_neg = calc_all(df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"40oMVGLClZUR","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy = (true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)\n","recall_val  = true_pos/(true_pos+false_neg+eps)\n","precision_val = true_pos/(true_pos+false_pos+eps)\n","f1_score = 2*precision_val*recall_val/(eps+precision_val+recall_val)\n","false_positive = false_pos/(true_neg+false_pos+eps)\n","fdr = false_pos/(true_pos+false_pos+eps)\n","print(\"accuracy = \", accuracy)\n","print(\"True positive rate /sensitivity, recall, hit rate = \", recall_val) \n","print(\"True negative rate specificity\", 1-false_positive)\n","print(\"precision = \",precision_val) \n","print(\"f1_score = \",f1_score) \n","print(\"False Discovery Rate\" , fdr)\n","print(\"false_positive = \", false_positive)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2roNzJqRlZUb","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","k = 20 adult dataset\n","training 0.79412902869147\n","testing 0.7953339230429014\n","\n","k = 15 adult dataset\n","training 0.79412902869147\n","testing 0.7953339230429014\n","\n","k = 10 adult dataset\n","0.79412902869147\n","0.7953339230429014\n","\n","k = 5 adult dataset\n","0.7514511581624191\n","0.7549756744803184\n","\n","'''\n","\n","\n","'''\n","k = 20 credit card\n","0.9905447447081072\n","0.9895070766227428\n","\n","k = 15 credit card\n","0.9903617397669737\n","0.9895070766227428\n","\n","k = 10 credit card\n","0.9884706887085951\n","0.9877989263055149\n","\n","k = 5 credit card\n","0.9883486854145062\n","0.9877989263055149\n","'''\n","\n","'''\n","k = 20 telco\n","0.7346467873624423\n","0.7348969438521677\n","\n","k = 15 telco\n","0.7346467873624423\n","0.7348969438521677\n","\n","k = 10 telco\n","0.7346467873624423\n","0.7348969438521677\n","\n","k = 5 telco\n","0.7346467873624423\n","0.7348969438521677\n","\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CLtEwvUIlZUm","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","techno train dataset with full depth\n","accuracy =  0.939297124600639\n","True positive rate /sensitivity, recall, hit rate =  0.888963210702\n","True negative rate specificity should be(1-false_positive) 0.111036789298\n","precision =  0.883056478405\n","f1_score =  0.886\n","False Discovery Rate 0.116943521595\n","false_positive =  0.0425223483933\n","\n","techno test dataset with full depth\n","accuracy =  0.7348969438521677\n","True positive rate /sensitivity, recall, hit rate =  0.498659517426\n","True negative rate specificity 0.501340482574\n","precision =  0.5\n","f1_score =  0.49932885906\n","False Discovery Rate 0.5\n","false_positive =  0.179883945841\n","\n","creditcard train dataset with full depth with same pos and neg\n","accuracy =  0.8944386090994527\n","True positive rate /sensitivity, recall, hit rate =  0.947583314942\n","True negative rate specificity 0.0524166850585\n","precision =  0.914970697922\n","f1_score =  0.930991489131\n","False Discovery Rate 0.0850293020778\n","false_positive =  0.266236654804\n","\n","creditcard test dataset with full depth with same pos and neg\n","accuracy =  0.8673469387755102\n","True positive rate /sensitivity, recall, hit rate =  0.908163265306\n","True negative rate specificity 0.0918367346939\n","precision =  0.839622641509\n","f1_score =  0.872549019608\n","False Discovery Rate 0.160377358491\n","false_positive =  0.173469387755\n","\n","creditcard train dataset with full depth pos and neg is 20000\n","accuracy =  0.9966449094125541\n","True positive rate /sensitivity, recall, hit rate =  0.911688311688\n","True negative rate specificity 0.0883116883117\n","precision =  0.943548387097\n","f1_score =  0.927344782034\n","False Discovery Rate 0.0564516129032\n","false_positive =  0.00131184407796\n","\n","creditcard test dataset with full depth pos and neg is 20000\n","accuracy =  0.9924353343094192\n","True positive rate /sensitivity, recall, hit rate =  0.841121495327\n","True negative rate specificity 0.158878504673\n","precision =  0.865384615385\n","f1_score =  0.85308056872\n","False Discovery Rate 0.134615384615\n","false_positive =  0.00350789275871\n","\n","adult train dataset with full depth \n","accuracy =  0.8944386090994527\n","True positive rate /sensitivity, recall, hit rate =  0.947583314942\n","True negative rate specificity 0.0524166850585\n","precision =  0.914970697922\n","f1_score =  0.930991489131\n","False Discovery Rate 0.0850293020778\n","false_positive =  0.266236654804\n","\n","adult test dataset with full depth \n","accuracy =  0.8329279080053074\n","True positive rate /sensitivity, recall, hit rate =  0.903925014646\n","True negative rate specificity 0.0960749853544\n","precision =  0.878326455102\n","f1_score =  0.890941898232\n","False Discovery Rate 0.121673544898\n","false_positive =  0.38583032491\n","'''"],"execution_count":0,"outputs":[]}]}