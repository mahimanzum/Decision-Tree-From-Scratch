{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecisionTreeOffline_with_multiple_cont_split.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Vdqe_iO2lY-q","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from numpy import log2 as log\n","from sklearn.preprocessing import Imputer\n","eps = np.finfo(float).eps\n","\n","\n","#print(eps)\n","dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n","       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n","       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n","       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}\n","\n","\n","\n","df = pd.DataFrame(dataset,columns= list(dataset.keys()))\n","#print(list(dataset.keys()))\n","\n","\n","def preProcess(df):\n","    imputer = Imputer(missing_values ='NaN', strategy = 'mean', axis = 0)\n","    df.drop('customerID', axis = 1, inplace=True)\n","    df.drop('TotalCharges', axis = 1, inplace=True)\n","    df['SeniorCitizen'] = df['SeniorCitizen'].astype(object)\n","    #print(df.head)\n","    print(df.dtypes)\n","    \n","    #imputer = imputer.fit(df)\n","    #imputer_data = Imputer.transform(df['tenure'].values)\n","    \n","df = pd.read_csv('telco.csv')\n","df.dropna(subset = ['Churn'])\n","\n","#df = pd.read_csv('creditcard.csv')\n","preProcess(df)\n","#print(df.head)\n","used = {}\n","uniqueFeaturesLables = {}\n","for attr in list(df.keys()):\n","    uniqueFeaturesLables[attr] = []\n","#print(df['Taste'])\n","\n","\n","for attr in list(df.keys()):\n","    for val in df[attr]:\n","        if(val not in uniqueFeaturesLables[attr]):\n","            uniqueFeaturesLables[attr].append(val)\n","'''\n","for attr in list(df.keys()):\n","    print(uniqueFeaturesLables[attr])\n","print(len(uniqueFeaturesLables))\n","'''\n","#df = pd.DataFrame(dataset)\n","print(type(df))\n","#print(df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0bns5dh_lY_e","colab_type":"code","colab":{}},"cell_type":"code","source":["def Entropy(df, attr = None):\n","    label = df.keys()[-1]\n","    values = df[label].unique()\n","    #print(values)\n","    entropy = 0.0\n","    \n","    for value in values:\n","        fraction = df[label].value_counts()[value]/len(df[label])\n","        entropy += -fraction*np.log2(fraction)\n","    if attr == None:\n","        return (entropy)\n","    entropy = 0.0\n","    final_entropy = 0\n","    attr_variables = df[attr].unique()\n","    for variable in attr_variables:\n","        entropy = 0.0\n","        for value in values:\n","            num = len(df[attr][df[attr] == variable][df[label] ==value])\n","            den = len(df[attr][df[attr] == variable])\n","            fraction = num/(den+eps)\n","            #print(den)\n","            entropy += -fraction*log(fraction+eps)\n","            fraction2 = den/len(df)\n","            final_entropy += fraction2*entropy\n","    #print(attr_variables)\n","    return (final_entropy)\n","\n","def ContEntropy(df, attr):\n","    label = df.keys()[-1]\n","    values = df[label].unique()\n","    entropy = 0.0\n","    total_yes = 0\n","    total_no = 0\n","    now_yes = 0\n","    now_no = 0\n","    total = 0\n","    for value in values:\n","        fraction = df[label].value_counts()[value]/len(df[label])\n","        if value == 'Yes':\n","            total_yes = df[label].value_counts()[value]\n","        else:\n","            total_no = df[label].value_counts()[value]\n","        entropy += -fraction*np.log2(fraction)\n","    total = total_yes+total_no\n","    entropy = 0.0\n","    final_entropy = 100\n","    partition_value = 0\n","    #attr_variables = df[attr].unique()\n","    attr_variables = []\n","    for i in range(len(df)):\n","        attr_variables.append((df[attr][i],df[label][i]))\n","    attr_variables = sorted(attr_variables,key = lambda element : element[0])\n","    #print((attr_variables))\n","    \n","    for i in range(len(attr_variables)):\n","        entropy = 0.0\n","        if(attr_variables[i][1]=='Yes'):\n","            now_yes += 1\n","        else:\n","            now_no += 1\n","        other_yes = total_yes - now_yes\n","        other_no = total_no - now_no\n","        if(i+1 != len(attr_variables) and attr_variables[i][0]==attr_variables[i+1][0]):\n","            continue\n","        entropy += -((now_yes+now_no)/(total+eps))*(((now_yes/(now_yes+now_no+eps))*log(now_yes/(now_yes+now_no+eps)))+((now_no/(now_yes+now_no+eps))*log(now_no/(now_yes+now_no+eps))))\n","        entropy += -((other_yes+other_no)/(total+eps))*(((other_yes/(other_yes+other_no+eps))*log(other_yes/(other_yes+other_no+eps)))+((other_no/(other_yes+other_no+eps))*log(other_no/(other_yes+other_no+eps))))\n","        if(entropy <= final_entropy):\n","            partition_value = attr_variables[i][0]\n","        final_entropy = min(final_entropy, entropy)\n","    '''\n","    \n","    for variable in attr_variables:\n","        entropy = 0.0\n","        for value in values:\n","            num = len(df[attr][df[attr] <= variable][df[label] ==value])\n","            den = len(df[attr][df[attr] <= variable])\n","            fraction = num/(den+eps)\n","            entropy += -fraction*log(fraction+eps)\n","        if(entropy <= final_entropy):\n","            partition_value = variable\n","        final_entropy = min(final_entropy, entropy)\n","    '''\n","    return final_entropy, partition_value\n","\n","def find_winner(df):\n","    #print(df.keys())\n","    #Entropy_att = []\n","    gain = []\n","    partition = []\n","    for key in df.keys()[:-1]:\n","        #print(df.dtypes[key])\n","        if df.dtypes[(key)] == 'object':\n","            #print(\"dhukse\")\n","            gain.append(Entropy(df)-Entropy(df,key))\n","            partition.append(\"Null\")\n","        else:\n","            #print(\"dhuke nai\")\n","            entrpy , parttn = ContEntropy(df,key)\n","            gain.append(Entropy(df)-entrpy)\n","            partition.append(parttn)\n","    idx =  np.argmax(gain)\n","    print(partition)\n","    return df.keys()[:-1][np.argmax(gain)],partition[idx] \n","  \n","def get_subtable(df,index,value):\n","    return df[df[index] == value].reset_index(drop=True)\n","\n","def get_subtable_cont(df,index,value):\n","    less_table = df[df[index] <= value].reset_index(drop=True)\n","    greater_table = df[df[index] > value].reset_index(drop=True)\n","    return less_table, greater_table\n","\n","find_winner(df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bqiT3ontlY_7","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def makeTree(df,depth, max_depth, tree = None, parent_class = None):\n","    Class = df.keys()[-1]\n","    #print(Class)\n","    #if(len(df[Class].unique())==1):\n","    #    return str(df[Class].unique()[0])\n","    #print(Class)\n","    partition_index, cont_partition_val = find_winner(df)\n","    #print(partition_index)\n","    #partition_values= np.unique(df[partition_index])\n","    partition_values = uniqueFeaturesLables[partition_index]\n","    if tree == None:\n","        tree = {}\n","        tree[partition_index] = {}\n","    #values = uniqueFeaturesLables[Class]\n","    values = df[Class].unique()\n","    #print(values)\n","    #print(values)\n","    x = 0\n","    majority_class = values[0]\n","    for value in values:\n","        #print(value)\n","        y = df[Class].value_counts()[value]\n","        if(y>x):\n","            x = y\n","            majority_class = value\n","    if depth==len(uniqueFeaturesLables) or depth == max_depth:\n","        return str(majority_class)\n","    if cont_partition_val == \"Null\":\n","        for value in partition_values:\n","            table = get_subtable(df, partition_index, value)\n","            ClassVal,cnt = np.unique(table[Class],return_counts=True)\n","            #print(\"ClassVal = \"+ ClassVal);\n","            if len(cnt) == 1:\n","                tree[partition_index][value] = ClassVal[0]\n","            elif(len(table) == 0):\n","                tree[partition_index][value] = str(majority_class)\n","            else:\n","\n","                tree[partition_index][value] = makeTree(table, depth+1, max_depth ,parent_class=majority_class)\n","    else:\n","        less_table, greater_table = get_subtable_cont(df, partition_index, cont_partition_val)\n","        less_ClassVal,less_cnt = np.unique(less_table[Class],return_counts=True)\n","        greater_ClassVal,greater_cnt = np.unique(less_table[Class],return_counts=True)\n","        done = 0\n","        #print(\"ClassVal = \"+ ClassVal);\n","        if(done == 0):\n","            if len(less_cnt) == 1:\n","                tree[partition_index][\"<=\"+str(cont_partition_val)] = less_ClassVal[0]\n","            elif(len(less_table) == 0):\n","                tree[partition_index][\"<=\" + str(cont_partition_val)] = str(majority_class)\n","            else:\n","                tree[partition_index][\"<=\" + str(cont_partition_val)] = makeTree(less_table, depth+1, max_depth ,parent_class=majority_class)\n","        if(done == 0):\n","            if len(greater_cnt) == 1:\n","                tree[partition_index][str(cont_partition_val)] = greater_ClassVal[0]\n","            elif(len(greater_table) == 0):\n","                tree[partition_index][str(cont_partition_val)] = str(majority_class)\n","            else:\n","                tree[partition_index][str(cont_partition_val)] = makeTree(greater_table, depth+1, max_depth ,parent_class=majority_class)\n","            \n","    return tree\n","    \n","    \n","tree = makeTree(df, depth = 0, max_depth = 100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TXEZzUOllZAX","colab_type":"code","colab":{}},"cell_type":"code","source":["import pprint\n","tree = makeTree(df, depth = 0, max_depth = 4)\n","pprint.pprint(tree)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k7BY5NmflZAv","colab_type":"code","colab":{}},"cell_type":"code","source":["import pprint\n","#tree = makeTree(df, depth = 0, max_depth = 2)\n","pprint.pprint(tree)\n","\n","unique_labels = df[df.keys()[-1]].unique()\n","unique_labels\n","print(unique_labels)\n","def predict(tree, inst):\n","    #pprint.pprint(tree)\n","    for nodes in tree.keys():\n","        if(nodes in unique_labels):\n","            return nodes\n","        value = inst[nodes]\n","        \n","        if df.dtypes[nodes] != 'object':\n","            #print(list(tree[nodes].keys())[0])\n","            \n","            if list(tree[nodes].keys())[0][0]=='<':\n","                less_value = list(tree[nodes].keys())[0]\n","                greater_value = list(tree[nodes].keys())[1]\n","            else:\n","                less_value = list(tree[nodes].keys())[1]\n","                greater_value = list(tree[nodes].keys())[0]\n","                \n","                \n","            \n","            if((df.dtypes[nodes] == 'int64' and inst[nodes] <= int(less_value[2:])) or (df.dtypes[nodes] == 'float64' and inst[nodes] <= float(less_value[2:]))):\n","                tree = (tree[nodes][less_value])\n","            else:\n","                tree = (tree[nodes][greater_value])\n","            \n","        \n","        #print(\"nodes = \" + nodes)\n","        #print(\"value = \" + value)\n","        #tree = (tree[nodes][value])\n","        \n","        \n","        prediction = 'No'\n","        if type(tree) is dict:\n","            prediction = predict(tree, inst)\n","        else:\n","            prediction = tree\n","            break;  \n","    return prediction\n","    if prediction=='Yes':\n","        return 1.0\n","    return -1.0\n","\n","inst = df.iloc[6]\n","#print(inst['Taste'])\n","#predict(tree, inst)\n","for i in range(10):\n","    print(predict(tree, df.iloc[i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZQOc2plvlZBB","colab_type":"code","colab":{}},"cell_type":"code","source":["#returns a vector of trees and weights of trees \n","from numpy.random import choice\n","feature_list = df.keys()\n","\n","def make_feedable(data):\n","    data_dic = {}\n","    for i in feature_list:\n","        data_dic[i] = []\n","    for j in feature_list:\n","        for i in data:\n","            data_dic[j].append(df[j][i])\n","    #print(data_dic)\n","    dic_df = pd.DataFrame(data_dic,columns= feature_list)\n","    return dic_df\n","\n","\n","def AdaBoost(df, K = 5):\n","    data_weights = {}\n","    Hypothesis_vector = []\n","    Hypothesis_weights = []\n","    list_of_candidates = []\n","    sampled = []\n","    for i in range(len(df)):\n","        data_weights[i] = 1.0/len(df)\n","        sampled.append(i)\n","        #print(data_weights[i])\n","    #print(data_weights.values())\n","    \n","    for j in range(len(df)):\n","        list_of_candidates.append(j)\n","        \n","    i = 0\n","    #change it\n","    new_df = df\n","    while (i<K):\n","        tree = makeTree(new_df, 0, 1)\n","        #print(sampled)\n","        error = 0.0\n","        for j in range(len(df)):\n","            prediction = predict(tree, df.iloc[j])\n","            if(prediction is not df[df.keys()[-1]][j]):\n","                error = error + data_weights[j]\n","                #print(error)\n","        if error>0.5:\n","            i = i-1\n","            #print(i)\n","            continue\n","        for j in range(len(df)):\n","            prediction = predict(tree, df.iloc[j])\n","            if(prediction is df[df.keys()[-1]][j]):\n","                data_weights[j] = (data_weights[j]*error)/(1-error)\n","        #normalization_step\n","        weight_array = list(data_weights.values())\n","        #print(type(weight_array[0]))\n","        weight_array = np.array(weight_array)\n","        #print(np.sum(weight_array))\n","        weight_array = weight_array/np.sum(weight_array)\n","        \n","        for j in data_weights.keys():\n","            data_weights[j] = weight_array[j]\n","        Hypothesis_weights.append(log((1-error)/error))\n","        Hypothesis_vector.append(tree)\n","        sampled = choice(list_of_candidates, int(0.4*len(df)), p=list(data_weights.values()))\n","        new_df = make_feedable(sampled)\n","        i = i + 1\n","        \n","    return Hypothesis_vector, Hypothesis_weights    \n","Hypothesis_vector, Hypothesis_weights = AdaBoost(df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CnXQ6SMblZBS","colab_type":"code","colab":{}},"cell_type":"code","source":["def final_prediction(Hypothesis_vector, Hypothesis_weights, instance):\n","    outpt = 0.0\n","    for i in range(len(Hypothesis_vector)):\n","        if(predict(Hypothesis_vector[i], instance) == 'Yes'):\n","           outpt = outpt + Hypothesis_weights[i]\n","        else:\n","           outpt = outpt - Hypothesis_weights[i]\n","    if outpt>=0:\n","        print(\"Yes\")\n","    else:\n","        print(\"No\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mWCe0cA5lZBm","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(10):\n","    final_prediction(Hypothesis_vector, Hypothesis_weights, df.iloc[i])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"axUcBDeTlZB_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}